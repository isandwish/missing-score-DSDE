{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArLngG7E6F-M"
      },
      "source": [
        "# Embedding data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL9uaGFijIsF"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import gdown\n",
        "import json\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "qygqsaftb2XR",
        "outputId": "5b277ccc-22c8-469a-aba2-21706b0ac310"
      },
      "outputs": [],
      "source": [
        "output = \"gdf_public_impact.csv\"\n",
        "url = \"https://drive.google.com/uc?id=1TWaXhd9-3PqjusF3lgyA_UKI2qwOE7mU\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "cleaned_df = pd.read_csv(output)\n",
        "cleaned_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGv_7nRijpJN",
        "outputId": "600ad4c4-ac93-41f5-9e81-e03a789bd96a"
      },
      "outputs": [],
      "source": [
        "# 1. Configure API\n",
        "GOOGLE_API_KEY = \"YOUR_API_KEY\"     # Insert your API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "# 2. Prepare sample data (random 10,000 rows)\n",
        "TARGET_ROWS = 10000\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "print(f\"Sampling {TARGET_ROWS} rows...\")\n",
        "\n",
        "# Ensure enough rows exist\n",
        "if len(cleaned_df) > TARGET_ROWS:\n",
        "    sample_df = cleaned_df.sample(n=TARGET_ROWS, random_state=42).reset_index(drop=True)\n",
        "else:\n",
        "    sample_df = cleaned_df.copy().reset_index(drop=True)\n",
        "\n",
        "print(f\"Sampled {len(sample_df)} rows. Ready to send in batches of {BATCH_SIZE}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9W6rBRQjCkX"
      },
      "outputs": [],
      "source": [
        "# 3. Batch prompt generation and API call\n",
        "def analyze_batch(batch_data):\n",
        "    # Convert batch input to JSON string\n",
        "    input_json = json.dumps(batch_data, ensure_ascii=False)\n",
        "\n",
        "    # Prompt template\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI assistant evaluating complaint reports (Traffy Fondue).\n",
        "    The output will be used as reference labels for training a machine-learning model.\n",
        "\n",
        "    Task:\n",
        "    Analyze the following comments (input is a JSON array):\n",
        "    {input_json}\n",
        "\n",
        "    Use the attached CSV reference (Risk Keywords) as guidance:\n",
        "    1. Identify whether the comment contains any keywords from the list.\n",
        "    2. Use the risk_score associated with matched keywords as the base score, but adjust based on context.\n",
        "    3. Assign an urgency score from 0 to 10.\n",
        "    4. Urgency levels: 0–3 = Low, 4–7 = Medium, 8–10 = High.\n",
        "    5. Issues that are commonly reported or long-standing should be capped at a maximum of 7 (Medium).\n",
        "    6. Scores 8–10 should be reserved only for severe, high-risk issues strongly aligned with risk keywords.\n",
        "    7. If the report is largely speculative (e.g., contains phrases like \"might\", \"maybe\"), prioritize objective causes.\n",
        "\n",
        "    Example guideline:\n",
        "    - High: Life/property danger, fire, collapse, severe hazards, large-scale impact.\n",
        "    - Medium: Moderate flooding, heavy traffic, damaged road, streetlight outage.\n",
        "    - Low: General issues, cleanliness, inquiries, non-critical reports.\n",
        "\n",
        "    Output JSON only:\n",
        "    {{\n",
        "        \"found_keywords\": [\"keyword1\", \"keyword2\"],\n",
        "        \"urgency\": \"High\" | \"Medium\" | \"Low\",\n",
        "        \"score\": 0–10\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        clean_text = (\n",
        "            response.text\n",
        "            .replace(\"```json\", \"\")\n",
        "            .replace(\"```\", \"\")\n",
        "            .strip()\n",
        "        )\n",
        "        return json.loads(clean_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        try:\n",
        "            print(f\"Server response preview: {response.text[:100]}\")\n",
        "        except:\n",
        "            print(\"No response text available.\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "45WGOsorigOj",
        "outputId": "7ed11f00-ccf4-4fc9-8753-680dab08f7d8"
      },
      "outputs": [],
      "source": [
        "# 4. Batch API loop\n",
        "results = []\n",
        "total_batches = math.ceil(len(sample_df) / BATCH_SIZE)\n",
        "\n",
        "print(f\"Starting batch processing: {total_batches} batches (rate limit: 10 RPM)\")\n",
        "\n",
        "for i in range(0, len(sample_df), BATCH_SIZE):\n",
        "\n",
        "    # Slice a batch of rows\n",
        "    batch = sample_df.iloc[i : i + BATCH_SIZE]\n",
        "\n",
        "    # Prepare input (use only ID and comment text)\n",
        "    batch_input = [\n",
        "        {\"id\": row[\"ticket_id\"], \"text\": row[\"comment\"]}\n",
        "        for _, row in batch.iterrows()\n",
        "    ]\n",
        "\n",
        "    # Call API\n",
        "    batch_result = analyze_batch(batch_input)\n",
        "\n",
        "    # Match results back to original rows\n",
        "    if batch_result:\n",
        "        for res in batch_result:\n",
        "            original_row = batch[batch[\"ticket_id\"] == res.get(\"id\")]\n",
        "\n",
        "            if not original_row.empty:\n",
        "                results.append({\n",
        "                    \"ticket_id\": res.get(\"id\"),\n",
        "                    \"comment\": original_row.iloc[0][\"comment\"],\n",
        "                    \"ai_urgency\": res.get(\"urgency\"),\n",
        "                    \"ai_score\": res.get(\"score\"),\n",
        "                })\n",
        "\n",
        "    print(f\"Batch {i // BATCH_SIZE + 1}/{total_batches} completed.\")\n",
        "\n",
        "    # Delay to respect rate limit (10 requests per minute)\n",
        "    # 10 RPM → 1 request every 6 seconds; allow 7 seconds for safety\n",
        "    time.sleep(2)\n",
        "\n",
        "# 5. Save result\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(\"traffy_gemini_batch.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"Completed. Total processed rows: {len(final_df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "xg2G1XaMDw-s",
        "outputId": "ef977b41-7bf2-40bc-f6fe-711025056168"
      },
      "outputs": [],
      "source": [
        "final_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lxe7vikE-1hw",
        "outputId": "57bf6cbc-cc00-47b2-bad5-cf30401fc24b"
      },
      "outputs": [],
      "source": [
        "# Configure pandas to display full text without truncation\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Ensure all columns are shown when printing DataFrames\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Preview the first 20 rows\n",
        "final_df.head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ooaq1NCLHe"
      },
      "source": [
        "# File Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FiA1PPOCN7Z",
        "outputId": "a5b235e6-3ea5-475c-f23d-6eab5f39d119"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "folder = \"data\"\n",
        "\n",
        "# Read all CSV files matching the pattern \"batch_*.csv\"\n",
        "files = glob.glob(\"batch_*.csv\")\n",
        "\n",
        "# Load and concatenate all files\n",
        "df_list = [pd.read_csv(f) for f in files]\n",
        "full_df = pd.concat(df_list, ignore_index=True)  # Reindex after concatenation\n",
        "\n",
        "print(f\"Merge completed. Total rows: {len(full_df)}\")\n",
        "\n",
        "# Save final merged dataset\n",
        "full_df.to_csv(os.path.join(folder, \"gemini_score_data.csv\"), index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
