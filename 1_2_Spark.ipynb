{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkti1qcxzoNA"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import json\n",
        "import pyspark\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "from shapely.geometry import Point\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import DoubleType, StructType, StructField, IntegerType, FloatType, StringType, BinaryType, ArrayType\n",
        "from pyproj import Transformer\n",
        "from pyspark.sql.window import Window\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "o2QSGnhOyfGa",
        "outputId": "51e3048d-26f7-4988-821f-15fb8df26506"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder\n",
        "        .master('local[*]')\n",
        "        .appName('Spark Tutorial')\n",
        "        .config('spark.ui.port', '4040')\n",
        "        .getOrCreate()\n",
        "        )\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJK0FxbdHcQd"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "kYUiSrAxztWH",
        "outputId": "644ea716-4c12-4d28-939b-e38ed7668811"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "folder = \"data\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "output = os.path.join(folder, \"bangkok_traffy.csv\")\n",
        "url = \"https://drive.google.com/uc?id=19QkF8i1my99gjbyHe7de_qZNwgrca6R5\"\n",
        "\n",
        "if not os.path.exists(output):\n",
        "    print(\"File not found. Downloading...\")\n",
        "    gdown.download(url, output, quiet=False)\n",
        "    print(\"Download completed!\")\n",
        "else:\n",
        "    print(\"File already exists. Skip downloading.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDNaWUEHrLDb"
      },
      "outputs": [],
      "source": [
        "manual_schema = StructType([\n",
        "    StructField(\"ticket_id\", StringType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"organization\", StringType(), True),\n",
        "    StructField(\"comment\", StringType(), True),\n",
        "    StructField(\"photo\", StringType(), True),\n",
        "    StructField(\"photo_after\", StringType(), True),\n",
        "    StructField(\"coords\", StringType(), True),\n",
        "    StructField(\"address\", StringType(), True),\n",
        "    StructField(\"subdistrict\", StringType(), True),\n",
        "    StructField(\"district\", StringType(), True),\n",
        "    StructField(\"province\", StringType(), True),\n",
        "    StructField(\"timestamp\", StringType(), True),\n",
        "    StructField(\"state\", StringType(), True),\n",
        "    StructField(\"star\", FloatType(), True),\n",
        "    StructField(\"count_reopen\", IntegerType(), True),\n",
        "    StructField(\"last_activity\", StringType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c8RdQW8rQIr",
        "outputId": "16ac619f-1540-4549-9fa4-9dbd54bab6ce"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(output, header=True, sep=',', quote='\"', escape='\"', multiLine=True, schema=manual_schema)\n",
        "print(df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c3b_YF3Fzuru",
        "outputId": "e158fd72-dae4-45f1-8553-e8ca426e16d3"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot6Nmfo-0OJr",
        "outputId": "e10feeab-81ca-4006-eb91-2541f25678ee"
      },
      "outputs": [],
      "source": [
        "(df.count(), len(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5q1Gcce4iDf"
      },
      "outputs": [],
      "source": [
        "# Drop unused columns\n",
        "cols_to_drop = [\n",
        "    'ticket_id', 'photo', 'photo_after', 'address',\n",
        "    'province', 'star', 'last_activity', 'organization',\n",
        "    'state', 'type', 'district', 'subdistrict'\n",
        "]\n",
        "\n",
        "df = df.drop(*cols_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPQ9iitW449E"
      },
      "outputs": [],
      "source": [
        "# Drop missing comment\n",
        "df = df.filter(col('comment').isNotNull())\n",
        "\n",
        "df = df.withColumn('comment', trim(col('comment').cast(\"string\")))\n",
        "\n",
        "# Drop empty comment\n",
        "df = df.filter(length(col('comment')) > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xjWAWsj5aUu"
      },
      "outputs": [],
      "source": [
        "# Standardize timestamp\n",
        "df = df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
        "\n",
        "# Drop invalid timestamp\n",
        "df = df.filter(col('timestamp').isNotNull())\n",
        "\n",
        "# Year 2022\n",
        "df = df.filter(year(col('timestamp')) == 2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhdX6zc9ZhtH",
        "outputId": "16d2e0fe-c63b-4da0-8ada-9f7a7ff4451d"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OFDjeOFZU1f",
        "outputId": "61df933b-3e3d-417d-fa87-ce91be8260e6"
      },
      "outputs": [],
      "source": [
        "# df[['lng', 'lat']] = df['coords'].str.split(',', expand=True).astype(float)\n",
        "df = df.withColumn('lng_str', split(col('coords'), ',').getItem(0)) \\\n",
        "       .withColumn('lat_str', split(col('coords'), ',').getItem(1))\n",
        "# geometry = [Point(xy) for xy in zip(df['lng'], df['lat'])]\n",
        "# df = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
        "\n",
        "df = df.withColumn('lng', col('lng_str').cast(DoubleType())) \\\n",
        "       .withColumn('lat', col('lat_str').cast(DoubleType()))\n",
        "\n",
        "# Drop coords\n",
        "# df = df.drop(columns=[\"coords\"], errors='ignore')\n",
        "df = df.drop('coords', 'lng_str', 'lat_str')\n",
        "\n",
        "# display(df.head())\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlh-Xhd35Xqw",
        "outputId": "337cea97-7d05-42aa-9eec-ce8a29f0e47a"
      },
      "outputs": [],
      "source": [
        "(df.count(), len(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIBt2yGFu9rJ",
        "outputId": "4102db79-3736-4200-8329-930e012ab1e3"
      },
      "outputs": [],
      "source": [
        "# 1. นิยาม Pandas UDF เพื่อสร้าง WKT String\n",
        "# @pandas_udf(StringType()) : กำหนดว่าฟังก์ชันนี้จะคืนค่าเป็น PySpark StringType\n",
        "@pandas_udf(StringType())\n",
        "def create_point_wkt(lng: pd.Series, lat: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    UDF นี้สร้าง Shapely Point และแปลงเป็น WKT String\n",
        "    (รูปแบบ POINT (X Y))\n",
        "    \"\"\"\n",
        "    # สร้างวัตถุ Point และแปลงเป็นรูปแบบ WKT (Well-Known Text)\n",
        "    # .wkt จะคืนค่าเป็น String ตามที่คุณต้องการ\n",
        "    return pd.Series([Point(x, y).wkt for x, y in zip(lng, lat)])\n",
        "\n",
        "# 2. ใช้งาน UDF กับ DataFrame\n",
        "# สร้างคอลัมน์ใหม่ชื่อ 'geometry_wkt'\n",
        "df = df.withColumn(\n",
        "    \"geometry_wkt\",\n",
        "    create_point_wkt(col(\"lng\"), col(\"lat\"))\n",
        ")\n",
        "\n",
        "# 3. ตรวจสอบผลลัพธ์\n",
        "df.select(\"lng\", \"lat\", \"geometry_wkt\").show(5, truncate=False)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtAm3QlY6Ra3",
        "outputId": "264dbe57-3a10-432e-a54c-6113029a4d90"
      },
      "outputs": [],
      "source": [
        "cleaned_df = df\n",
        "\n",
        "# จำนวนแถว\n",
        "print(\"Number of rows:\", cleaned_df.count())\n",
        "\n",
        "# จำนวนคอลัมน์\n",
        "print(\"Number of columns:\", len(cleaned_df.columns))\n",
        "\n",
        "# schema ของแต่ละคอลัมน์\n",
        "cleaned_df.printSchema()\n",
        "\n",
        "# สรุป non-null ของแต่ละคอลัมน์\n",
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "cleaned_df.select([count(col(c)).alias(c) for c in cleaned_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2f-ERta6YZY",
        "outputId": "44a21d58-6ff8-4bff-d303-0b69d413f59b"
      },
      "outputs": [],
      "source": [
        "# Check is null\n",
        "# cleaned_df.isna().sum()\n",
        "\n",
        "columns_to_check = cleaned_df.columns\n",
        "\n",
        "# สร้าง Expression สำหรับการนับค่า Null ในแต่ละคอลัมน์\n",
        "# when(col(c).isNull(), 1).otherwise(0) หมายถึง:\n",
        "# ถ้าคอลัมน์นั้นเป็น Null ให้มีค่าเป็น 1, ไม่อย่างนั้นให้มีค่าเป็น 0\n",
        "# จากนั้นใช้ sum() เพื่อรวมค่า 1 เหล่านั้น\n",
        "null_counts_expr = [\n",
        "    sum(when(isnull(col(c)), 1).otherwise(0)).alias(c)\n",
        "    for c in columns_to_check\n",
        "]\n",
        "\n",
        "# คำนวณและแสดงผลลัพธ์\n",
        "null_counts_df = cleaned_df.select(null_counts_expr)\n",
        "\n",
        "# แสดงผลลัพธ์ (โดยปกติจะมีเพียง 1 แถว)\n",
        "null_counts_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faKda2UkZKrF"
      },
      "source": [
        "## count_reopen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8og-YCKD-Wg",
        "outputId": "4d889347-06b7-4854-fbae-8acb4e47234e"
      },
      "outputs": [],
      "source": [
        "# percentiles = [0.90, 0.95, 0.97, 0.98, 0.99, 0.995, 0.999]\n",
        "# cleaned_df['count_reopen'].quantile(percentiles)\n",
        "\n",
        "# List ของ Percentiles ที่คุณต้องการคำนวณ\n",
        "percentiles = [0.90, 0.95, 0.97, 0.98, 0.99, 0.995, 0.999]\n",
        "\n",
        "# สร้าง Expression สำหรับการคำนวณ Percentiles\n",
        "# percentile_approx(column, array_of_percentiles, accuracy)\n",
        "percentile_expr = percentile_approx(\n",
        "    col('count_reopen'),\n",
        "    array([lit(p) for p in percentiles]), # สร้าง Array ของค่า Percentile\n",
        "    100000 # ค่าความแม่นยำ (ยิ่งสูงยิ่งแม่นยำ, 100000 เป็นค่าที่แนะนำ)\n",
        ").alias('percentile_values')\n",
        "\n",
        "# ใช้นำมาคำนวณและแสดงผลลัพธ์\n",
        "percentile_df = cleaned_df.select(percentile_expr)\n",
        "\n",
        "# แสดงผลลัพธ์\n",
        "percentile_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qltXTWLYxDKe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtDX5DuxC-9s"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "GbZfDbwl9drX",
        "outputId": "d58a52a1-9cea-4e48-a383-4b7e84726f06"
      },
      "outputs": [],
      "source": [
        "# ดึงคอลัมน์จาก Spark DF มาเป็น pandas Series\n",
        "count_reopen_pd = cleaned_df.select(\"count_reopen\").toPandas()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.hist(count_reopen_pd['count_reopen'], bins=50)\n",
        "plt.title(\"Distribution of count_reopen (Before)\")\n",
        "plt.xlabel(\"count_reopen\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "xfklQ_Cs9e4k",
        "outputId": "8dd3bc7c-cc54-405e-d31f-99af5d2980c4"
      },
      "outputs": [],
      "source": [
        "count_reopen_pd = cleaned_df.select(\"count_reopen\").toPandas()\n",
        "\n",
        "plt.figure(figsize=(6,2))\n",
        "plt.boxplot(count_reopen_pd['count_reopen'], vert=False)\n",
        "plt.title(\"Boxplot of count_reopen (Before)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6r_RRFS-ozY"
      },
      "outputs": [],
      "source": [
        "cleaned_df = cleaned_df.withColumn(\n",
        "    \"count_reopen_log\",\n",
        "    log1p(col(\"count_reopen\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "pnn-JRsA_Cxc",
        "outputId": "16f802ca-1208-4b7d-bf29-573570ffdf9a"
      },
      "outputs": [],
      "source": [
        "count_reopen_log_pd = cleaned_df.select(\"count_reopen_log\").toPandas()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.hist(count_reopen_log_pd['count_reopen_log'], bins=50)\n",
        "plt.title(\"Distribution of count_reopen_log (After)\")\n",
        "plt.xlabel(\"count_reopen_log\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "8AvF45hMEMyU",
        "outputId": "3f9e25fa-acd0-45fe-872e-36308f3f40f3"
      },
      "outputs": [],
      "source": [
        "count_reopen_log_pd = cleaned_df.select(\"count_reopen_log\").toPandas()\n",
        "\n",
        "plt.figure(figsize=(6,2))\n",
        "plt.boxplot(count_reopen_log_pd['count_reopen_log'], vert=False)\n",
        "plt.title(\"Boxplot of count_reopen_log (After)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtrT_wwiESHE",
        "outputId": "2c7e8952-15cf-4264-c263-694cd9717eab"
      },
      "outputs": [],
      "source": [
        "percentiles = [0.90, 0.95, 0.97, 0.98, 0.99, 0.995, 0.999]\n",
        "\n",
        "result = cleaned_df.approxQuantile(\n",
        "    \"count_reopen\",\n",
        "    percentiles,\n",
        "    0.0   # relative error = 0 (exact)\n",
        ")\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgHrIjJEEe9T",
        "outputId": "e6650056-10e2-4487-8cc9-71c2a827f7c9"
      },
      "outputs": [],
      "source": [
        "cleaned_df.select(skewness(\"count_reopen\")).show()\n",
        "cleaned_df.select(skewness(\"count_reopen_log\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMiAFVbmZIoD",
        "outputId": "99725fc2-4eba-4b69-e26b-dd1c69d7fbbb"
      },
      "outputs": [],
      "source": [
        "num_rows = cleaned_df.count()\n",
        "num_cols = len(cleaned_df.columns)\n",
        "print(\"Shape:\", (num_rows, num_cols))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPFL0ldT4vXH",
        "outputId": "153de111-8ca4-47ab-9128-cd1302365ada"
      },
      "outputs": [],
      "source": [
        "cleaned_df.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp1Nj7d21h6g",
        "outputId": "cb8b66b3-00af-446e-eda6-02192e8e6138"
      },
      "outputs": [],
      "source": [
        "for col_name in cleaned_df.columns:\n",
        "    print(col_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUq6HYjkJvNi"
      },
      "source": [
        "# Web scraping\n",
        "impact_to_public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HS6alUXMerc",
        "outputId": "a06ac27d-5052-4f92-9476-64bdd18e76f1"
      },
      "outputs": [],
      "source": [
        "# Department Store\n",
        "\n",
        "url_department = \"https://data.bangkok.go.th/dataset/d8f814ac-cbaf-43c3-9576-f533b2554776/resource/438101c3-5535-4fe2-bc5e-83aa73703d4a/download/department_store.csv\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "response = requests.get(url_department, headers=headers)\n",
        "response.raise_for_status()\n",
        "\n",
        "with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp:\n",
        "    tmp.write(response.content)\n",
        "    temp_path = tmp.name\n",
        "\n",
        "df_department = spark.read.csv(temp_path, header=True, inferSchema=True)\n",
        "\n",
        "df_department.printSchema()\n",
        "df_department.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-2-bKvn2nki",
        "outputId": "e3e8e9b1-6836-4cfa-92e8-122c4c68afa8"
      },
      "outputs": [],
      "source": [
        "# Community\n",
        "url_community = \"https://cpudgiapp.bangkok.go.th/arcgis/rest/services/Community/Service_Community_Public/MapServer/0/query\"\n",
        "params = {\n",
        "    \"where\": \"1=1\",\n",
        "    \"outFields\": \"*\",\n",
        "    \"f\": \"json\",\n",
        "    \"returnGeometry\": \"true\"\n",
        "}\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "response = requests.get(url_community, headers=headers, params=params)\n",
        "response.raise_for_status()\n",
        "data = response.json()\n",
        "\n",
        "features = [f[\"attributes\"] for f in data[\"features\"]]\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\") as tmp:\n",
        "    for feature in features:\n",
        "        tmp.write(json.dumps(feature) + \"\\n\")\n",
        "    temp_path = tmp.name\n",
        "\n",
        "df_community = spark.read.json(temp_path)\n",
        "df_community.printSchema()\n",
        "df_community.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg5Z5_jn9tWN",
        "outputId": "1d064b09-4c84-4dd5-8a8d-63a3fa2898fa"
      },
      "outputs": [],
      "source": [
        "#school\n",
        "url_school = \"https://bmagis.bangkok.go.th/arcgis/rest/services/riskbkk/RISK_ADMIN_bma_school/FeatureServer/0/query\"\n",
        "\n",
        "params = {\n",
        "    \"where\": \"1=1\",\n",
        "    \"outFields\": \"*\",\n",
        "    \"f\": \"json\",\n",
        "    \"returnGeometry\": \"true\"\n",
        "}\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "response = requests.get(url_school, headers=headers, params=params)\n",
        "response.raise_for_status()\n",
        "data = response.json()\n",
        "\n",
        "features = [f[\"attributes\"] for f in data[\"features\"]]\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\") as tmp:\n",
        "    for feature in features:\n",
        "        tmp.write(json.dumps(feature) + \"\\n\")\n",
        "    temp_path = tmp.name\n",
        "\n",
        "df_school= spark.read.json(temp_path)\n",
        "df_school.printSchema()\n",
        "df_school.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnsgzHvVAh15",
        "outputId": "5f636487-0741-4580-dd86-9d05b71d0a7b"
      },
      "outputs": [],
      "source": [
        "# Hospital\n",
        "url_hospital = \"https://bmagis.bangkok.go.th/arcgis/rest/services/riskbkk/RISK_ADMIN_Hospital/FeatureServer/0/query\"\n",
        "\n",
        "# ใช้พารามิเตอร์เหมือน Community/School\n",
        "params = {\n",
        "    \"where\": \"1=1\",\n",
        "    \"outFields\": \"*\",\n",
        "    \"f\": \"json\",\n",
        "    \"returnGeometry\": \"true\"\n",
        "}\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "# ดึงข้อมูล JSON\n",
        "response = requests.get(url_hospital, headers=headers, params=params)\n",
        "response.raise_for_status()\n",
        "data_hospital = response.json()\n",
        "\n",
        "# Extract attributes\n",
        "features = [f[\"attributes\"] for f in data_hospital[\"features\"]]\n",
        "\n",
        "# บันทึกเป็นไฟล์ชั่วคราว JSON Lines (เหมือน CSV ใน pattern เดิม)\n",
        "with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\") as tmp:\n",
        "    for feature in features:\n",
        "        tmp.write(json.dumps(feature) + \"\\n\")\n",
        "    temp_path = tmp.name\n",
        "\n",
        "# โหลดเข้า Spark\n",
        "df_hospital = spark.read.json(temp_path)\n",
        "df_hospital.printSchema()\n",
        "df_hospital.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA8eBLHxW0hc"
      },
      "outputs": [],
      "source": [
        "def clean_and_convert_to_gdf(df, col_map, place_type, drop_zero=True):\n",
        "    # Step 1: select & rename\n",
        "    df_clean = df.select(\n",
        "        F.col(col_map[\"name\"]).alias(\"name\"),\n",
        "        F.col(col_map[\"lat\"]).alias(\"lat\"),\n",
        "        F.col(col_map[\"lng\"]).alias(\"lng\")\n",
        "    )\n",
        "\n",
        "    # Step 2: clean name\n",
        "    df_clean = df_clean.withColumn(\"name\", F.trim(F.col(\"name\").cast(StringType())))\n",
        "    df_clean = df_clean.filter((F.col(\"name\").isNotNull()) & (F.col(\"name\") != \"\"))\n",
        "\n",
        "    # Step 3: convert numeric\n",
        "    df_clean = df_clean.withColumn(\"lat\", F.col(\"lat\").cast(DoubleType()))\n",
        "    df_clean = df_clean.withColumn(\"lng\", F.col(\"lng\").cast(DoubleType()))\n",
        "\n",
        "    # Step 4: drop missing / zero\n",
        "    df_clean = df_clean.filter(F.col(\"lat\").isNotNull() & F.col(\"lng\").isNotNull())\n",
        "    if drop_zero:\n",
        "        df_clean = df_clean.filter((F.col(\"lat\") != 0) & (F.col(\"lng\") != 0))\n",
        "\n",
        "    # Step 5: detect coordinate system\n",
        "    max_lat = df_clean.agg(F.max(\"lat\")).collect()[0][0]\n",
        "\n",
        "    if max_lat > 1000:\n",
        "        # เป็น UTM → ต้อง convert เป็น WGS84\n",
        "        print(f\"[{place_type}] Detected UTM coordinates → converting to WGS84...\")\n",
        "\n",
        "        transformer = Transformer.from_crs(\"EPSG:32647\", \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "        def utm_to_wgs84(lng, lat):\n",
        "            x, y = transformer.transform(lng, lat)\n",
        "            return float(y), float(x)\n",
        "\n",
        "        schema = StructType([\n",
        "            StructField(\"lat\", DoubleType(), True),\n",
        "            StructField(\"lng\", DoubleType(), True)\n",
        "        ])\n",
        "        convert_udf = udf(utm_to_wgs84, schema)\n",
        "        df_clean = df_clean.withColumn(\"coords\", convert_udf(F.col(\"lng\"), F.col(\"lat\")))\n",
        "        df_clean = df_clean.withColumn(\"lat\", F.col(\"coords.lat\")).withColumn(\"lng\", F.col(\"coords.lng\"))\n",
        "        df_clean = df_clean.drop(\"coords\")\n",
        "\n",
        "    # Step 6: create geometry column (WKT)\n",
        "    df_clean = df_clean.withColumn(\"geometry\", F.concat(F.lit(\"POINT(\"), F.col(\"lng\"), F.lit(\" \"), F.col(\"lat\"), F.lit(\")\")))\n",
        "\n",
        "    # Step 7: add type column\n",
        "    df_clean = df_clean.withColumn(\"type\", F.lit(place_type))\n",
        "\n",
        "    return df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li_xrMdBUphJ"
      },
      "outputs": [],
      "source": [
        "column_mapping = {\n",
        "    \"department\": {\"name\": \"name\", \"lat\": \"lat\", \"lng\": \"lng\"},\n",
        "    \"community\":  {\"name\": \"CMT_NAME\", \"lat\": \"LAT\",  \"lng\": \"LON\"},\n",
        "    \"school\":     {\"name\": \"NAME\",     \"lat\": \"Y\",    \"lng\": \"X\"},\n",
        "    \"hospital\":   {\"name\": \"NAME\",     \"lat\": \"Y\",    \"lng\": \"X\"},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHYz6iDkYEmE",
        "outputId": "7f79de06-2701-4f27-f8c1-087d4e40826e"
      },
      "outputs": [],
      "source": [
        "gdf_department = clean_and_convert_to_gdf(\n",
        "    df_department, column_mapping[\"department\"], \"department\"\n",
        ")\n",
        "\n",
        "gdf_community = clean_and_convert_to_gdf(\n",
        "    df_community,  column_mapping[\"community\"],  \"community\"\n",
        ")\n",
        "\n",
        "gdf_school = clean_and_convert_to_gdf(\n",
        "    df_school,     column_mapping[\"school\"],     \"school\"\n",
        ")\n",
        "\n",
        "gdf_hospital = clean_and_convert_to_gdf(\n",
        "    df_hospital,   column_mapping[\"hospital\"],   \"hospital\"\n",
        ")\n",
        "gdf_hospital.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgK_5M9UYGQ8",
        "outputId": "3cf18d54-3283-4a80-f8d5-b6855404697b"
      },
      "outputs": [],
      "source": [
        "# รวมทุกประเภทเป็น DataFrame เดียว\n",
        "gdf_public_place = gdf_department\\\n",
        "    .unionByName(gdf_community) \\\n",
        "    .unionByName(gdf_school) \\\n",
        "    .unionByName(gdf_hospital)\n",
        "\n",
        "# นับจำนวนแถวทั้งหมด\n",
        "print(\"Total public places:\", gdf_public_place.count())\n",
        "windowSpec = Window.orderBy(monotonically_increasing_id())\n",
        "gdf_public_place = gdf_public_place.withColumn(\"index\", row_number().over(windowSpec))\n",
        "cols = gdf_public_place.columns  # ดึงชื่อคอลัมน์ทั้งหมด\n",
        "cols.remove(\"index\")             # ลบ index ออกจาก list\n",
        "cols = [\"index\"] + cols          # นำ index มาไว้ด้านหน้า\n",
        "\n",
        "# เลือกคอลัมน์ตามลำดับใหม่\n",
        "gdf_public_place = gdf_public_place.select(*cols)\n",
        "\n",
        "gdf_public_place.show()\n",
        "# แสดงตัวอย่าง 5 แถวแรก\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psPZn-ZSYHyJ",
        "outputId": "fe6a8f5e-6eae-4171-f908-9d1331ad158a"
      },
      "outputs": [],
      "source": [
        "gdf_public_place.sample(fraction=0.01, seed=42).show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4WvplCwHP8w",
        "outputId": "0c5bc0b8-1979-469a-91c5-f7299a7fce74"
      },
      "outputs": [],
      "source": [
        "cleaned_df = cleaned_df.withColumn(\"index\", row_number().over(windowSpec))\n",
        "cols = cleaned_df.columns  # ดึงชื่อคอลัมน์ทั้งหมด\n",
        "cols.remove(\"index\")             # ลบ index ออกจาก list\n",
        "cols = [\"index\"] + cols          # นำ index มาไว้ด้านหน้า\n",
        "\n",
        "# เลือกคอลัมน์ตามลำดับใหม่\n",
        "cleaned_df = cleaned_df.select(*cols)\n",
        "\n",
        "cleaned_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND8HOrHXbSuR"
      },
      "outputs": [],
      "source": [
        "copy_cleaned_df = cleaned_df\n",
        "copy_gdf_public_place = gdf_public_place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfvWarrRdHew",
        "outputId": "64943d44-98d3-4696-d57a-df896976b9c9"
      },
      "outputs": [],
      "source": [
        "copy_gdf_public_place.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI7Xmd9RdgzH",
        "outputId": "6039295f-488a-4e94-86d0-094026ba1d10"
      },
      "outputs": [],
      "source": [
        "copy_cleaned_df\n",
        "# print(copy_gdf_public_place.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-vaIccUcWHp"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, lit, array\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from math import radians, sin, cos, sqrt, atan2, log1p\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, lit\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from math import radians, sin, cos, sqrt, atan2, log1p\n",
        "\n",
        "def compute_public_impact(\n",
        "    gdf_cases,\n",
        "    gdf_places,\n",
        "    max_distance=1000,\n",
        "    weights={\"school\":0.3,\"department\":0.2,\"community\":0.2,\"hospital\":0.3}\n",
        "):\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    place_types = list(weights.keys())\n",
        "\n",
        "    print(\"เริ่มฟังก์ชัน compute_public_impact\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Haversine function\n",
        "    # -----------------------------\n",
        "    def haversine(lat1, lon1, lat2, lon2):\n",
        "        R = 6371000  # meters\n",
        "        phi1, phi2 = radians(lat1), radians(lat2)\n",
        "        dphi = radians(lat2 - lat1)\n",
        "        dlambda = radians(lon2 - lon1)\n",
        "        a = sin(dphi/2)**2 + cos(phi1)*cos(phi2)*sin(dlambda/2)**2\n",
        "        c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
        "        return R * c\n",
        "\n",
        "    # -----------------------------\n",
        "    # Prepare places dict\n",
        "    # -----------------------------\n",
        "    places_dict = {}\n",
        "    for t in place_types:\n",
        "        places_list = gdf_places.filter(col(\"type\") == t) \\\n",
        "            .select(\"lat\", \"lng\") \\\n",
        "            .rdd.map(lambda row: (row.lat, row.lon)) \\\n",
        "            .collect()\n",
        "        places_dict[t] = places_list\n",
        "    print(\"เตรียม places dict:\", places_dict)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Compute distances UDF\n",
        "    # -----------------------------\n",
        "    def compute_distances(case_lat, case_lon):\n",
        "        result = {}\n",
        "        for t, locations in places_dict.items():\n",
        "            dists = []\n",
        "            for plat, plon in locations:\n",
        "                d = haversine(case_lat, case_lon, plat, plon)\n",
        "                if d <= max_distance:\n",
        "                    dists.append(d)\n",
        "            result[t] = dists\n",
        "        return [result[t] for t in place_types]\n",
        "\n",
        "    compute_distances_udf = udf(compute_distances, ArrayType(ArrayType(DoubleType())))\n",
        "    gdf_cases = gdf_cases.withColumn(\"distances_array\", compute_distances_udf(col(\"lat\"), col(\"lon\")))\n",
        "    print(\"คำนวณ distances_array เรียบร้อย\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Split distances_array into <type>_distances\n",
        "    # -----------------------------\n",
        "    for i, t in enumerate(place_types):\n",
        "        gdf_cases = gdf_cases.withColumn(f\"{t}_distances\", col(\"distances_array\")[i])\n",
        "    gdf_cases = gdf_cases.drop(\"distances_array\")\n",
        "    print(\"แยก distances array เป็น <type>_distances\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Inverse distance → _point_scores\n",
        "    # -----------------------------\n",
        "    def inverse_distance(dists):\n",
        "        return [max(0, max_distance - d) for d in dists] if dists else []\n",
        "\n",
        "    inverse_distance_udf = udf(inverse_distance, ArrayType(DoubleType()))\n",
        "    for t in place_types:\n",
        "        gdf_cases = gdf_cases.withColumn(f\"{t}_point_scores\", inverse_distance_udf(col(f\"{t}_distances\")))\n",
        "    print(\"คำนวณ _point_scores เรียบร้อย\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Raw score → log1p(sum)\n",
        "    # -----------------------------\n",
        "    def raw_score(dists):\n",
        "        return log1p(sum(dists)) if dists else 0\n",
        "\n",
        "    raw_score_udf = udf(raw_score, DoubleType())\n",
        "    for t in place_types:\n",
        "        gdf_cases = gdf_cases.withColumn(f\"{t}_raw_score\", raw_score_udf(col(f\"{t}_point_scores\")))\n",
        "    print(\"คำนวณ _raw_score เรียบร้อย\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Normalize 0–100 → _score\n",
        "    # -----------------------------\n",
        "    for t in place_types:\n",
        "        max_val = gdf_cases.agg({f\"{t}_raw_score\": \"max\"}).collect()[0][0]\n",
        "        if max_val > 0:\n",
        "            gdf_cases = gdf_cases.withColumn(f\"{t}_score\", col(f\"{t}_raw_score\") / max_val * 100)\n",
        "        else:\n",
        "            gdf_cases = gdf_cases.withColumn(f\"{t}_score\", lit(0))\n",
        "    print(\"Normalize _score เป็น 0–100 เรียบร้อย\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Weighted sum → public_impact\n",
        "    # -----------------------------\n",
        "    gdf_cases = gdf_cases.withColumn(\"public_impact\", lit(0.0))\n",
        "    for t, w in weights.items():\n",
        "        gdf_cases = gdf_cases.withColumn(\"public_impact\", col(\"public_impact\") + col(f\"{t}_score\") * w)\n",
        "    print(\"คำนวณ public_impact เรียบร้อย\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Show intermediate results (ตัวอย่าง)\n",
        "    # -----------------------------\n",
        "    print(\"แสดงผลตัวอย่าง 5 แถวสุดท้าย:\")\n",
        "    rows = gdf_cases.select(\"lat\",\"lng\",\"public_impact\", *[f\"{t}_score\" for t in place_types]).collect()\n",
        "    for row in rows[:5]:  # แสดง 5 แถวแรก\n",
        "        print(row)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Drop temp columns\n",
        "    # -----------------------------\n",
        "    temp_cols = [c for c in gdf_cases.columns if any(x in c for x in [\"_distances\",\"_point_scores\",\"_raw_score\",\"_score\"]) and c != \"public_impact\"]\n",
        "    gdf_cases = gdf_cases.drop(*temp_cols)\n",
        "\n",
        "    print(\"เสร็จสิ้น compute_public_impact\")\n",
        "    return gdf_cases\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fB9ySziNgXj6",
        "outputId": "02f40157-4ab9-469d-ccb7-8870853d1957"
      },
      "outputs": [],
      "source": [
        "gdf_public_impact = compute_public_impact(copy_cleaned_df, copy_gdf_public_place)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UyYEiMmgvYn"
      },
      "outputs": [],
      "source": [
        "# เรียงตาม public_impact จากมากไปน้อย\n",
        "gdf_public_impact.orderBy(F.col(\"public_impact\").desc()).show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ib2UrF7iXbn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# เรียงจากมากไปน้อย\n",
        "sorted_df = gdf_public_impact.orderBy(F.col(\"public_impact\").desc())\n",
        "\n",
        "# สุ่ม 10 แถวจาก DataFrame ที่เรียงแล้ว\n",
        "sampled_df = sorted_df.orderBy(F.rand()).limit(10)\n",
        "\n",
        "# แสดงผล\n",
        "sampled_df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZoW2Ndli69N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ดึงคอลัมน์ public_impact ออกมาเป็น Pandas Series\n",
        "public_impact_values = gdf_public_impact.select(\"public_impact\").toPandas()[\"public_impact\"]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(\n",
        "    public_impact_values,\n",
        "    bins=30,\n",
        "    edgecolor=\"black\"\n",
        ")\n",
        "plt.title(\"Distribution of Public Impact Score\", fontsize=16)\n",
        "plt.xlabel(\"Public Impact Score\", fontsize=14)\n",
        "plt.ylabel(\"Frequency\", fontsize=14)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCCthrNhjvxj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# คำนวณ skewness ของคอลัมน์ public_impact\n",
        "skew_value = gdf_public_impact.select(F.skewness(\"public_impact\")).collect()[0][0]\n",
        "\n",
        "print(skew_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQZPZF7BjBWX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# แปลง Spark DataFrame เป็น Pandas DataFrame (ถ้า dataset ใหญ่ให้ sample)\n",
        "pdf = gdf_public_impact.select(\"lng\", \"lat\", \"public_impact\").toPandas()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(\n",
        "    pdf[\"lng\"],\n",
        "    pdf[\"lat\"],\n",
        "    c=pdf[\"public_impact\"],\n",
        "    cmap=\"viridis\",\n",
        "    s=10\n",
        ")\n",
        "plt.colorbar(label=\"Public Impact\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"Spatial Distribution of Public Impact\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TfZU6jZjUJs"
      },
      "outputs": [],
      "source": [
        "# ลบคอลัมน์ geometry\n",
        "df_export = gdf_public_impact.drop(\"geometry\")\n",
        "\n",
        "# เขียนเป็น CSV\n",
        "df_export.write.csv(\"gdf_public_impact.csv\", header=True, mode=\"overwrite\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4acqHBE9jVqO"
      },
      "outputs": [],
      "source": [
        "gdf_public_impact.to_file(\"gdf_public_impact.geojson\", driver=\"GeoJSON\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENChGK5UJ1Lo"
      },
      "source": [
        "# Mini LLM\n",
        "mini_llm_risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7STm4OHJ2vn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGJfEn8_HZBB"
      },
      "source": [
        "# AI/ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7au5s20Gvz2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dsde-cedt-project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
